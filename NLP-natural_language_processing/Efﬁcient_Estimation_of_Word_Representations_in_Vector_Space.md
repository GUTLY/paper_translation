## [单词在向量空间的有效估计](https://arxiv.org/abs/1301.3781)

### 标题：Efficient Estimation of Word Representations in Vector Space
### 作者：Tomas Mikolow, Kai Chen, Greg Corrado, Jeffrey Dean
## Google

#### 摘要

我们建议两个新的模型架构为非常大的数据集计算单词的连续向量量表示。这些表示的优劣使用单词相似度测试来衡量，研究结果与之前基于不同类型神经网络的最佳表现技术进行了比较。我们观察到在精度上的巨大的提高和更低的计算开销等。它可以在一天以内从有16亿单词的数据集中学得高质量的词向量。此外，我们展示了这些向量在使用测试数据集衡量句法和语义词之间的相似度时的最好的性能表现。

#### 1 简介

当前许多NLP系统和技术都把单词当作原子单位来处理-在单词之间没有相似度的概念，因为它们在词汇表中表示为索引。这个选择有一些很好的理由，简单、健壮，同时简单模型训练大规模数据比复杂数据训练少量数据的标现更好。其中一个例子就是颇受欢迎且如今被当作统计语言模型的N-gram模型，它使训练几乎所有可用的数据（数万亿单词）成为可能。

然而，在很多任务上简单模型有自身的局限。例如，用于语音自动识别的相关数据是有限的，模型的表现通常由高质量的手抄演讲数据（通常只有数百万单词）的大小决定。在机器学习上，现存的许多语言的语料库只包含十亿或更少的单词。因此，在这种简单扩展基础技术的情形下将不会有任何的重大进步，于是我们不得不关注于更加高级的技术。

随着近年来机器学习技术的发展，使得在非常大的数据集上训练更复杂模型成为可能，且它们通常比简单模型表现得更好。大概其中最成功的概念便是单词的分布式表示。例如，基于语言模型的神经网络的表现的远比N-gram模型更好。

##### 1.1 本文目标