## 深度学习

### 原文：[Deep learning](https://creativecoding.soe.ucsc.edu/courses/cs523/slides/week3/DeepLearning_LeCun.pdf)
### 作者：Yann LeCun, Yoshua Bengio, Geoffrey Hinton
#### 2015
### 目录：

[TOC]

#### 1 摘要

深度学习（deep learning）允许由多个处理层组成的计算模型学习具有多个抽象级别的数据表示（representation）。这些方法显著的提高了语音识别（speech recognition）、视觉对象识别（visual object recognition）、对象检测（object detection）和许多其他的领域（如药物发现和基因组学）的最佳表现。深度学习通过使用反向传播算法（backpropagation algorithm）去指示机器如何改变内部参数（internal parameters）来发现大数据集的复杂结构，这些内部参数被用于从上一层的表示中计算每一层的表示。深度卷积网络（deep convolutional nets）带来了图像（images）、视频（video）、语音（speech）和音频（audio）处理的重大突破，而递归网络（recurrent nets）则在文本（text）和语音（speech）等序列数据（sequential data）方面取得了突破。

#### 2 概述

机器学习技术驱动了现代社会的许多方面：从网络搜索到社交网络（social netwroks）的内容过滤，再到电子商务网站（e-commerce websites)的推荐。它越来越多的出现在相机和智能手机这样的消费产品中。机器学习系统被用于识别图像中的物体，将语音转换维文本，将新闻条目、帖子或产品与用户的兴趣进行匹配，并选择搜索的相关结果。这些应用越来越多的使用一种名为深度学习的技术。

传统的机器学习技术（conventional machine-learning techniques）处理原始数据的能力有限。数十年以来，构造一个模式识别（ pattern-recognition）或者机器学习系统需要详细的工程设计和相当多的领域专业知识来设计一个特征提取器（feature extractor），将原始数据（如图像的像素值）转换成合适的内部表示（internal representation）或特征向量（feature vector），使得学习子系统（通常是一个分类器）可以从中检测（detect）或分类（classify）输入中的模式（input）。

表示学习（representation learning）是一套学习方法，它允许机器从原始数据中自动发现检测或分类所需要的表示。深度学习方法是多层表示的表示学习方法，通过组合简单但非线性的模型获得，每个模型从一个层次（从原始输入开始）转换为更高、更抽象层次的表示。

深度学习方法是一种具有多层表示的表示学习方法，通过组合简单但非线性的模块获得，每个模块将一个级别的表示(从原始输入开始)转换为一个更高、更抽象的表示。通过足够的变换组合，可以学得非常复杂的函数。对于分类任务来说，高层次的表示放大了输入的某些方面，这些方面在区别和抑制不相关变量非常重要。例如，一张图片由一个像素数组组成。

更高层次的表现放大了输入的某些方面，而这些方面对于辨别和抑制无关的变化非常重要。而第一层表示中的习得特征（features）通常表示图像中特定方向和位置边缘的存在或不存在。第三层将图案组合为更大的组合，这些组合对应于熟悉对象（familiar objects）的部分，随后的层将检测这些部分组合的对象。深度学习的关键在于，这些层的特征不是由人类工程师设计的：它们使用数据从多用途的学习程序中习得。

深度学习在解决多年来一直限制人工智能界最佳尝试的问题方面取得了重大进展。它非常擅长发现高维数据的复杂结构，因此它被用于像科学、商业和政府的许多领域。另外，它在图像识别 **[1-4]** 和语音识别 **[5-7]** 上也有很好的表现，它在预测药物分子的潜在活性 **[8]**、分析粒子加速器（particle accelerator)数据 **[9, 10]**、重建大脑回路 **[11]** 和预测非编码DNA突变对基因表达和疾病的影响 **[12, 13]** 的这些方面上打败了其他的机器学习算法。令人更惊讶的是，深度学习算法非常有希望解决各种各样的自然语言理解的问题 **[14]**，尤其是主体分类（topic classificaiton）、情感分析（sentiment analysis）、问题回答 **[15]** 和语言翻译 **[16, 17]**。

我们认为深度学习在不久的将来将会取得更多的成功。因为它只需要非常少的手工操作，所以它可以很容易的利用增加的计算能力和数据。当前正在发展的的、新的、深度神经网络（deep neural networks）的学习算法和架构将会加快这个进程。

#### 3 监督学习 Supervised learning

无论是不是深度学习，监督学习都是机器学习的最常见形式。想象一下，我们想要构建一个可以根据图像内容来分类图像的系统，然后告诉我们，这是一栋房子、一辆车、一个人或者宠物。我们首先收集一个很大的数据集，其中包括房子、车、人和宠物的图像，并对每个图像进行分类。经过训练，向机器展示一张图片，它对每个类别输出所产生向量得分。我们期望在目标分类在每个分类的得分中有最高的分数，但这个在训练以前不太可能发生。我们使用目标函数（objective function）来衡量输出得分和期望的分数模式之间的误差（或距离）。接着机器修改其内部的、可调的参数来减少这个误差。这些可调参数通常被成为权重（weight），它们是一个真实数据（real numbers），且可以被认为是机器输入-输出函数的“旋钮”。在典型的深度学习系统中，可能会有数亿这样的可调参数以及训练这个机器的数亿被标记后的样例。

为了合适的调整权重向量，学习算法为每个权重计算了一个梯度（gradient）向量，用以表示权重微小数据的增加所带来的误差的增加或减少。然后将权重向量调整到与梯度向量相反的方向。

目标函数对所有训练样本求得平均值，可以看作是高维权重空间中的一种丘陵景观。反向的梯度向量指出了在该景观中最陡的下降方向，使其更加的接近最小值，在这里平均输出的误差较小。

在实际操作中，大多数人使用一种被成为随机梯度下降（stochastic gradient descent，SGD）的方法来处理。随机梯度下降包括显示一些样本的输入向量，计算输出和误差，计算这些样本的平均梯度并以此适当的调整权重。这个过程在训练集中的许多小样本示例中不断重复，直到目标函数的平均值停止下降。它之所以被称为随机，是因为每个小样本都给出了所有样本平均梯度的噪声估计（noisy estimate）。这个简单的方法经常要比更加复杂的优化方法更快的找到一组好的权重 **[18]**。经过训练后的程序的性能通常使用一个不同的、被称为测试集的样本集合来衡量。这有助于测试机器的泛化（generalization）能力——对在训练过程中从未出现过的输入产生合理输出的能力。

当前，许多机器学习的实际应用都是基于手动设置的特征的线性分类器。两类线性分类器（two-class linear classifier）计算特征向量分量（components）的加权和。如果加权和超过了阈值，就将输入分到一个特定的类别。

从20世纪60年代以来，我们发现线性分类器只可以将输入空间分割为非常简单的区域，即超平面（hyperplane）分割的半空间（half-spaces）**[19]**。但是像图片和语音识别等问题要求输入-输出函数对某些输入的变化不敏感，例如位置的变化、物体的方向或光强（illumination）或者语音的音量和口音的变化。同时它要对某些细微的变化非常敏感（例如，白狼和萨摩耶犬这样的白色类狼的狗之间的区别）。在像素水平上，两张萨摩耶犬在不同姿势和不同环境下的图像可能有非常大的区别。然而，两张萨摩耶犬和白狼在相同位置和背景下的图像可能非常相似。一个线性分类器或者任何一个“浅层”（shallow）分类器在处理原始像素上无法区分后两者，将把它与前两者归为同一类别。这就是为什么浅层分类器需要一个好的特征提取器，用它来处理选择-不变性困境（selectivity-invariance dilemma），即生成对图像的重要方面具有选择性的表示，但对不相关的方面（如动物的姿势）具有不变性。为了增强线性分类器的能力,可以使用一般的非线性特征，如核方法（kernel methods）**[20]**，但是一般的特征，比如高斯核（Gaussian kernel）产生的特征，不允许学习者在远离训练示例的地方进行泛化 **[21]**。传统的选择是手动设计一个好的特征提取器，为此需要相当大数量的工程技术和专业知识。但是如果可以使用一个通用的学习程序来自动的学得这些特征，这一切都可以避免。这就是深度学习得核心优势。

深度学习架构（architecture）是简单模块的多层堆栈（multilayer stack），所有模块（或大多数模块）都需要学习，并且其中许多模块都计算非线性输入输出映射。堆栈中的每个模块都会转换其输入，用以增加表示的选择性和不变性。一个具有多个非线性层（例如深度为5到20）的系统，它可以对输入实现非常复杂的函数，这些函数对微小细节敏感（区分萨摩耶犬和白狼）的同时又对大的无关变量不敏感（例如，背景、姿势、灯光和周围物体）。

####  4 反向传播来训练多层架构 

从最早的模式识别（pattern recognititon）开始 **[22, 23]** ，研究者的目标就是使用可训练的多层网络来取代手动设置的特征。尽管其非常的简单，但直到20世纪80年代中期它才得到了广泛的解决。正如后来的发现，多层架构可以使用简单的随机梯度下降法来训练。只要模型的输入和内部权重是相对平滑的函数，就可以使用反向传播算法来计算梯度。这个可以实现并且是有用的方法，是由几个不同的团队在20世纪70年代到80年代独立发现的 **[24-27]**。

反向传播过程用于计算目标函数相对于模型多层堆栈权重的梯度，无非是导数链式法则的实际应用。其中的关键在于，目标关于模型输入的导数（或梯度）可以使用该模型输出（或后续模型的输入）的梯度逆向计算得到（图 1）。

 ![figure 1](D:\machine\paper_translation\images\deep_learning\fig1.png)

图 1|多层神经网络合反向传播：**a**，一个多层神经网络（由连接圆点表示）可以扭曲输入空间，使数据的类别可以被线性分离（例如在红蓝线上的数据）。注意，输入（input）空间的规则网格（如左侧所示）如何通过隐藏（hidden）单元转换（如中间面板所示）。这是一个只有两个输入单元，两个隐藏单元和一个输出单元的说明示例， 但是在对象识别或自然语言处理的神经网络包括成千上万个单元。经[C.Olah](http://colah.github.io/)允许转载。**b**， 导数的链式法则告诉我们两个小的影响( x 对 y 的小改变和 y 对 z 的小改变)是如何构成的。通过 x 的一个微小改变Δx通过乘以 ∂y/∂x 首先转化为y的微小改变 Δy（偏微分的定义）。相似的，Δy 创造了 z 的改变 Δz。将一个方程带入另一个方程可得出导数的链式法则，Δx 通过乘以 ∂y/∂x 和 ∂z/∂x 转换为 Δz。当x，y，z为向量时它也适用（导数是Jacobian矩阵）。**c**，这个方程被用于计算含有两个隐藏层和一个输出层得神经网络的前向路径（forward pass）。其中每个方程构成一个模块，通过该模块可以反向传播梯度。在每一层，我们首先计算每个单元的总输入 z，它是下一层单元输出的加权和。然后将非线性函数 *f*  应用于 z上，得到单元的输出。为了叙述简洁，我们忽略了偏置（bias）。这个非线性模型被用于神经网络，包括近年来常用的整流线性单位（ReLU）*f*(z)=max(0, z)以及更常见的S型曲线（sigmoids），例如双曲线正切（hyberbolic tangent）*f*(z)=（exp(z) - exp(-z)/exp(z) + exp(-z))和逻辑函数（logistic function）logistic ，f(z)= 1/(1 + exp(-z)) 。**d**,这个方程被用于计算反向路径（backward pass）。













